---
title: "CMA-ES Glide Path Optimizer"
subtitle: "Bernstein Polynomial Parameterization -- N-Asset Structurally Smooth Allocations"
author: "Thomas Benacci"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    code_folding: hide
    fig_width: 10
    fig_height: 8
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 10, fig.height = 8, cache = FALSE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(MASS)
library(lhs)
library(parallel)
```

# Motivation

This project grew out of a simple thought experiment that I kept returning to: the conventional wisdom for retirement investing is to shift out of equities and into bonds as you get older, locking in gains and reducing exposure to downside risk. That makes sense if your only goal is not running out of money. But most people have a second goal that pulls in the opposite direction: they want to leave something behind. A bequest target, whether it is a house paid off, a trust, or just a meaningful sum for children or grandchildren, requires growth. Growth means equities. And that creates a genuine tension, as the desire to secure invested returns for retirement conflicts with the desire to reach a target bequest for relatives.

The standard target-date fund answer is a monotone glide path that slopes downward from high equity to low equity over your lifetime. That is a fine heuristic, but it is only one shape among many. What if it were actually better, under certain mortality and spending assumptions, to re-balance back into equities at some point late in life? What if accepting more volatility when you are old enough that the probability of surviving to spend the money is small, but the upside for your heirs is large? Intuitively this seems plausible, but I had no computationally feasible framework in my mind for discovering what that optimal shape actually looks like. You cannot just try every possible allocation at every age.

The idea that eventually clicked was: do not optimize the allocations directly. Instead, optimize the shape of a curve that defines the allocations. If that curve is a polynomial, you only need to optimize a handful of coefficients, and the polynomial handles the interpolation to every year of life. The problem collapses from ~80 free weights to maybe 10-20 control points per asset class. Once I started reading about Bernstein polynomials and their remarkable geometric properties (guaranteed smoothness, guaranteed boundedness, shape-preserving behavior) I realized they woudl work great for this application. The optimizer, CMA-ES (Covariance Matrix Adaptation Evolution Strategy) is an evolutionary solver that works well in this moderate dimensionality, no gradient, and noisy evaluation problem.

This notebook discovers near-optimal lifetime allocation glide paths across all asset classes simultaneously by minimizing the probability of ruin, P(ruin), using CMA-ES operating on Bernstein polynomial control points. Each asset class gets its own independent Bernstein curve with N_CP control points, and at each age the raw curve values are normalized to sum to one so they form valid portfolio weights. The optimizer is free to find any smooth shape for each asset and the answer it converges on reflects the real trade-off between safety and growth given a specific set of mortality, income, spending, and bequest assumptions.


# Background: Bernstein Polynomials

A Bernstein polynomial is a way of expressing a curve as a weighted blend of basis functions. Given n control points b_0, b_1, ..., b_{n-1}, each in [0,1], the degree-(n-1) Bernstein polynomial evaluated at a parameter tau in [0,1] is:

$$w(\tau) = \sum_{k=0}^{n-1} \binom{n-1}{k} \tau^k (1-\tau)^{n-1-k} \cdot b_k$$

In plain terms: at each value of tau (which we map to age), the curve is a weighted average of the control points, where the weights come from the binomial distribution. Early in life (tau near 0), the curve is dominated by the first few control points. Late in life (tau near 1), the last few control points dominate. In between, the influence blends smoothly.

Several properties make Bernstein polynomials ideal for glide path optimization. First, smoothness is structural: the curve is infinitely differentiable by construction, so there is no way to produce a jagged or discontinuous allocation path no matter what values the control points take. This means we never need a roughness penalty in the objective function because the parameterization itself rules out bad shapes. Second, the convex hull property guarantees that the curve always lies within the convex hull of its control points. Since we constrain each control point to [0,1], the resulting raw values are automatically in [0,1] at every age with no clamping or projection needed. Third, the search space is low-dimensional: instead of optimizing one free weight per year of life per asset class, we optimize only N_CP control points per asset, which dramatically improves convergence speed and solution quality for any black-box optimizer. Fourth, despite the smoothness guarantee, the polynomials are not restricted to any particular shape. Declining, rising, U-shaped, S-shaped, and flat paths are all reachable. The basis enforces smoothness, not monotonicity. Fifth, for numerical stability we use the de Casteljau recurrence rather than evaluating the polynomial using the binomial formula directly, which involves large binomial coefficients that can cause floating-point issues. The de Casteljau algorithm evaluates the polynomial by repeated linear interpolation and is both numerically stable and easy to implement. It is the same algorithm used in computer graphics to render Bezier curves, which are themselves Bernstein polynomials.


# Background: CMA-ES

CMA-ES (Covariance Matrix Adaptation Evolution Strategy) is a derivative-free optimization algorithm designed for continuous, non-convex, and potentially noisy objective functions. It belongs to the family of evolutionary strategies, but it is much more sophisticated than a simple genetic algorithm.

Each generation, CMA-ES samples a population of candidate solutions (called lambda) from a multivariate normal distribution centered on the current best estimate of the optimum. The candidates are evaluated on the objective function, ranked, and the best half (called mu) are used to update the distribution for the next generation.

The defining feature of CMA-ES is that it maintains and adapts a covariance matrix that describes the shape of its search distribution. Over time, the algorithm learns which directions in the search space are promising (those that consistently lead to improvement) and stretches the sampling distribution along those directions. This is loosely analogous to learning a local approximation of the inverse Hessian, but without ever computing a gradient.

Separately from the covariance matrix, CMA-ES maintains a global step size (sigma) that controls the overall scale of the search. Sigma is adapted using a mechanism called cumulative step-size adaptation (CSA), which tracks whether consecutive steps are correlated (suggesting sigma is too small) or anti-correlated (suggesting sigma is too large). When the algorithm converges, sigma collapses toward zero.

In this notebook we use the separable variant, sep-CMA-ES, which maintains only the diagonal of the covariance matrix rather than the full matrix. This reduces the per-generation cost from O(n^2) to O(n), which matters when running tens of thousands of Monte Carlo simulations per fitness evaluation. The trade-off is that sep-CMA-ES cannot learn correlations between dimensions, but for Bernstein control points (which are relatively independent) this is an acceptable simplification.

The mu best candidates are not averaged equally. Instead, CMA-ES uses log-weighted recombination: the best candidate gets the most weight, the second-best gets less, and so on. The effective number of parents (mu_eff) summarizes how concentrated the weighting is. This accelerates convergence compared to equal-weight averaging.

The overall loop each generation is: sample candidates from the current distribution, evaluate fitness in parallel, rank by fitness, update the mean (weighted average of best candidates), update the evolution paths, adapt the covariance matrix diagonal, and adapt sigma. The algorithm terminates when sigma collapses below a threshold, when the best fitness stalls for a specified number of generations, or when the maximum number of generations is reached.


# Asset Class Parameters

We model four asset classes with real (inflation-adjusted) return parameters calibrated to 1927-2006 US market history. Returns are drawn from a multivariate lognormal distribution: log(1+R) ~ N(mu, Sigma). The log-space mean is derived from the arithmetic mean and volatility via the standard adjustment: mu_log = log(1 + mu_arith) - sigma^2 / 2. The correlation matrix captures the key empirical relationships: domestic and international equities are moderately correlated with each other (0.60), bonds and equities are nearly uncorrelated, and cash is slightly negatively correlated with international equities. The chunk below defines these parameters and constructs the covariance matrix by scaling the correlation matrix by the asset-specific standard deviations. The number of asset classes (N_ASSETS) is derived from the length of asset_names, so adding or removing asset classes here automatically propagates through the rest of the notebook.

```{r asset_params}
asset_names <- c("Cash", "IntBond", "DomEq", "IntlEq")
N_ASSETS    <- length(asset_names)
mu_arith    <- c(0.0081, 0.0252, 0.0948, 0.0725)
sigma       <- c(0.0410, 0.0614, 0.2099, 0.2159)
mu_log      <- log(1 + mu_arith) - sigma^2 / 2

cor_mat <- matrix(c(
  1.00,  0.40,  0.02, -0.05,
  0.40,  1.00,  0.05,  0.02,
  0.02,  0.05,  1.00,  0.60,
 -0.05,  0.02,  0.60,  1.00
), N_ASSETS, N_ASSETS)

cov_mat <- diag(sigma) %*% cor_mat %*% diag(sigma)

cat("Asset classes:", paste(asset_names, collapse = ", "), "\n")
cat("N assets:     ", N_ASSETS, "\n")
cat("Arithmetic real returns:", paste(sprintf("%.2f%%", mu_arith * 100), collapse = ", "), "\n")
cat("Standard deviations:   ", paste(sprintf("%.2f%%", sigma * 100), collapse = ", "), "\n")
```

# User Parameters

These are the personal financial assumptions that define the lifecycle simulation. They include current age, retirement age, starting savings, income and its real growth rate, the savings rate during working years, annual spending in retirement, Social Security benefits and their start age, a bequest target (the amount we hope to leave behind), and a maximum acceptable ruin probability. The mortality table is loaded from an external file produced by a separate mortality prediction model. If that file is not found, the code falls back to a Gompertz hazard model as a reasonable approximation.

```{r user_params}
# -- Mortality
mortality_path <- "F:/PredictingMortality/mc_mortality_input.rds"

# -- Personal finances
my_current_age        <- 30
my_retirement_age     <- 65
my_current_savings    <- 100000
my_current_income     <- 85000
my_savings_rate       <- 0.12
my_real_income_growth <- 0.01
my_annual_spending    <- 80000
my_ss_benefit         <- 24000
my_ss_start_age       <- 67
my_bequest_target     <- 1500000
my_max_ruin_prob      <- 0.08
```

# CMA-ES and Bernstein Parameters

This chunk sets the hyperparameters that control the optimizer and the polynomial parameterization. N_CP is the number of Bernstein control points per asset class, and the total dimensionality of the search space is N_ASSETS * N_CP since each asset gets its own independent curve. CMAES_POP_SIZE (lambda) is the number of candidate solutions sampled per generation. CMAES_MAX_GEN caps the number of generations, and CMAES_TOL together with CMAES_STALL_LIM control early stopping: the optimizer halts if the best fitness does not improve by more than TOL for STALL_LIM consecutive generations, or if sigma collapses below 1e-5. CMAES_SIGMA0 is the initial step size in control-point space, and since control points live in [0,1], a starting sigma of 0.25 means the algorithm initially explores a range of roughly +/- 0.25 around the initial guess. N_MC is the number of Monte Carlo paths used to evaluate each candidate solution and is held fixed throughout the entire run, with no phase switching or escalation, so that fitness comparisons between candidates and across generations are always apples-to-apples.

```{r cmaes_params}
# -- Bernstein polynomial
N_CP           <- 20     # number of control points PER ASSET (= polynomial degree + 1)
N_DIMS_TOTAL   <- N_ASSETS * N_CP  # total optimization dimensions

# -- CMA-ES
CMAES_POP_SIZE  <- 40    # lambda; slightly larger for higher dimensionality
CMAES_MAX_GEN   <- 300   # more generations for the larger search space
CMAES_TOL       <- 1e-7
CMAES_SIGMA0    <- 0.25  # initial step size in control-point space [0,1]
CMAES_STALL_LIM <- 40

# -- Single fixed MC budget (no phase switching)
N_MC    <- 50000   # LHS paths per fitness evaluation -- fixed for the entire run

N_CORES <- max(1L, detectCores() - 1L)

cat(sprintf("Asset classes:            %d (%s)\n", N_ASSETS, paste(asset_names, collapse=", ")))
cat(sprintf("Control points per asset: %d\n", N_CP))
cat(sprintf("Total optimization dims:  %d\n", N_DIMS_TOTAL))
cat(sprintf("Population size:          %d\n", CMAES_POP_SIZE))
cat(sprintf("Max generations:          %d\n", CMAES_MAX_GEN))
cat(sprintf("Initial sigma:            %.2f\n", CMAES_SIGMA0))
cat(sprintf("MC paths (fixed):         %s\n", format(N_MC, big.mark = ",")))
cat(sprintf("Parallel cores:           %d\n", N_CORES))
```

# Load Mortality CDF

This chunk loads a personalized mortality table from an external .rds file. The mortality table provides, for each age, the probability of dying at that age and the cumulative probability of having died by that age. These probabilities drive the simulation: each Monte Carlo path is assigned a random death age drawn from this distribution, and the simulation for that path ends at that age. If the external mortality file is not found, the code falls back to a parametric Gompertz hazard model, which models the force of mortality as an exponential function of age: h(x) = a * exp(b * (x - 30)). The parameters a = 0.00003 and b = 0.085 produce a reasonable approximation of adult mortality in developed countries. The plot shows the cumulative mortality CDF with reference lines for the median death age and the retirement age.

```{r load_mortality, fig.width=9, fig.height=5}
if (file.exists(mortality_path)) {
  mc_mort        <- readRDS(mortality_path)
  mort_table     <- mc_mort$mortality_table
  my_current_age <- mc_mort$current_age

  cat(sprintf("Loaded personalized mortality from: %s\n", mortality_path))
  cat(sprintf("Current age:        %d\n", my_current_age))
  cat(sprintf("Median death age:   %.1f\n", mc_mort$median_death_age))
  cat(sprintf("Expected death age: %.1f\n", mc_mort$expected_death_age))
} else {
  cat("WARNING:", mortality_path, "not found. Using Gompertz fallback.\n")
  ages <- my_current_age:110
  a <- 0.00003; b <- 0.085
  haz  <- a * exp(b * (ages - 30))
  surv <- cumprod(1 - pmin(haz, 1))
  dp   <- c(1, surv[-length(surv)]) - surv
  dp   <- pmax(dp, 0); dp <- dp / sum(dp)
  mort_table <- data.frame(
    age = ages, survival_prob = surv,
    death_prob = dp, cum_death_prob = cumsum(dp)
  )
  mc_mort <- list(
    current_age        = my_current_age,
    median_death_age   = ages[which.min(abs(cumsum(dp) - 0.5))],
    expected_death_age = sum(ages * dp),
    sd_death_age       = sqrt(sum((ages - sum(ages * dp))^2 * dp)),
    mortality_table    = mort_table
  )
}

max_age       <- max(mort_table$age)
n_years_total <- max_age - my_current_age

ggplot(mort_table, aes(x = age, y = cum_death_prob)) +
  geom_line(color = "firebrick", linewidth = 1.2) +
  geom_vline(xintercept = mc_mort$median_death_age, linetype = "dashed") +
  geom_vline(xintercept = my_retirement_age, linetype = "dotted", color = "steelblue") +
  annotate("text", x = mc_mort$median_death_age + 1, y = 0.55,
           label = paste0("Median death: ", round(mc_mort$median_death_age, 1)),
           hjust = 0, size = 3.5) +
  annotate("text", x = my_retirement_age + 1, y = 0.05,
           label = paste0("Retirement: ", my_retirement_age),
           hjust = 0, color = "steelblue", size = 3.5) +
  scale_y_continuous(labels = percent_format()) +
  labs(title = "Personalized Mortality CDF",
       x = "Age", y = "Cumulative Probability of Death") +
  theme_minimal(base_size = 14)

cat(sprintf("\nLifespan horizon: age %d to %d (%d years)\n",
            my_current_age + 1, max_age, n_years_total))
```

# Bernstein Polynomial Engine

This chunk implements the core polynomial machinery. de_casteljau() evaluates a Bernstein polynomial at a single point tau in [0,1] using the de Casteljau recurrence. The algorithm works by repeated linear interpolation: starting with the n control points, it performs n-1 rounds of pairwise blending, each round reducing the number of active values by one, until a single scalar remains. This is the value of the polynomial at tau. The procedure is numerically stable because it never computes large binomial coefficients directly. bernstein_path() applies de_casteljau() at evenly spaced values of tau across the full lifespan, producing a vector of values, one per year of life.

The new function bernstein_weight_paths() is the generalization to N asset classes. It takes a flat candidate vector of length N_ASSETS * N_CP, reshapes it into one set of N_CP control points per asset, expands each set into a full-lifespan path via bernstein_path(), and then normalizes the K raw values at each year so they sum to one. The result is a matrix of dimension n_years_total x N_ASSETS, where each row is a valid portfolio weight vector. Because each raw curve is in [0,1] (by the convex hull property) and we normalize by dividing by the sum, the weights are guaranteed to be non-negative and to sum to one at every age, with no additional constraints needed.

```{r bernstein}
# Stable evaluation of degree-(n-1) Bernstein polynomial at scalar tau in [0,1]
de_casteljau <- function(beta, tau) {
  b <- beta
  n <- length(b)
  for (r in 1:(n - 1)) {
    b[1:(n - r)] <- (1 - tau) * b[1:(n - r)] + tau * b[2:(n - r + 1)]
  }
  b[1]
}

# Evaluate at every year of life; returns vector of length n_years
bernstein_path <- function(beta, n_years = n_years_total) {
  tau_seq <- seq(0, 1, length.out = n_years)
  sapply(tau_seq, function(tau) de_casteljau(beta, tau))
}

# Generalized: flat beta vector -> n_years x N_ASSETS weight matrix (rows sum to 1)
bernstein_weight_paths <- function(beta_flat, n_assets = N_ASSETS, n_cp = N_CP,
                                   n_years = n_years_total) {
  beta_flat <- pmin(pmax(beta_flat, 0), 1)
  # Reshape: each asset gets n_cp consecutive control points
  raw_paths <- matrix(NA_real_, nrow = n_years, ncol = n_assets)
  for (k in seq_len(n_assets)) {
    idx <- ((k - 1) * n_cp + 1):(k * n_cp)
    raw_paths[, k] <- bernstein_path(beta_flat[idx], n_years)
  }
  # Normalize rows to sum to 1 (all raw values are in [0,1] by convex hull)
  row_sums <- rowSums(raw_paths)
  row_sums[row_sums == 0] <- 1  # safety: avoid division by zero
  raw_paths / row_sums
}

# Illustrate with a few hand-picked examples
demo1 <- c(rep(0.8, N_CP), rep(0.6, N_CP), rep(0.9, N_CP), rep(0.3, N_CP))
demo2 <- c(seq(0.9, 0.2, length.out = N_CP), seq(0.2, 0.8, length.out = N_CP),
           seq(0.5, 0.9, length.out = N_CP), seq(0.3, 0.5, length.out = N_CP))

demo_w1 <- bernstein_weight_paths(demo1)
demo_w2 <- bernstein_weight_paths(demo2)

demo_plot_df <- bind_rows(
  data.frame(age = (my_current_age + 1):max_age, demo_w1, shape = "Flat starting weights") %>%
    setNames(c("age", asset_names, "shape")),
  data.frame(age = (my_current_age + 1):max_age, demo_w2, shape = "Trending weights") %>%
    setNames(c("age", asset_names, "shape"))
) %>%
  pivot_longer(cols = all_of(asset_names), names_to = "asset", values_to = "weight") %>%
  mutate(asset = factor(asset, levels = asset_names))

ggplot(demo_plot_df, aes(x = age, y = weight, fill = asset)) +
  geom_area(alpha = 0.8) +
  geom_vline(xintercept = my_retirement_age, linetype = "dashed", color = "grey40") +
  facet_wrap(~shape) +
  scale_y_continuous(labels = percent_format()) +
  scale_fill_brewer(palette = "Set2") +
  labs(title = sprintf("Example N-Asset Bernstein Glide Paths (%d assets, %d CPs each)",
                       N_ASSETS, N_CP),
       subtitle = "Any smooth trajectory per asset is reachable -- the optimizer discovers the best combination",
       x = "Age", y = "Portfolio Weight", fill = "") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

# Pre-generate Returns (Latin Hypercube) and Death Ages

Before the optimization begins, this chunk generates the entire stochastic environment that every candidate solution will be evaluated against: a matrix of asset returns for every simulation path and every year, and a vector of death ages.

The reason for pre-generating everything is that if we drew fresh random returns for every fitness evaluation, two candidates evaluated in the same generation would be compared on different random draws. Random noise would contaminate the fitness rankings, and the optimizer would waste generations chasing phantom improvements. By fixing the random draws once and reusing them for every evaluation in every generation, we guarantee that fitness differences between candidates reflect genuine differences in their glide paths, not sampling luck.

Rather than plain Monte Carlo (independent uniform draws), we use Latin Hypercube Sampling (LHS) to generate the random inputs. LHS partitions each dimension of the input space into N_MC equal-probability strata and places exactly one sample in each stratum. This produces much more even coverage of the input space than simple random sampling, which tends to leave gaps and clusters. The result is lower variance in estimated statistics (like P(ruin)) for the same number of simulations, or equivalently, the same accuracy with fewer simulations.

The procedure works as follows: generate a Latin Hypercube of uniform samples with N_MC rows and (N_ASSETS * n_years) columns, transform the uniform samples to standard normal via the inverse CDF (qnorm), apply the Cholesky factor of the covariance matrix to introduce cross-asset correlations, shift by the log-space means, then exponentiate and subtract 1 to get arithmetic returns. The return generator is fully generic over the number of asset classes and returns a 3D array of dimension (N_MC x n_years x N_ASSETS). Death ages are sampled by drawing a uniform random number for each simulation path and inverting the mortality CDF. The verification block at the end compares the realized sample means to the target arithmetic means to confirm the LHS draws are correct.

```{r pregenerate}
sample_death_ages <- function(n, mort_table) {
  u <- runif(n)
  sapply(u, function(p) {
    idx <- which(mort_table$cum_death_prob >= p)[1]
    if (is.na(idx)) max(mort_table$age) else mort_table$age[idx]
  })
}

generate_lhs_returns <- function(n_sims, n_years, n_assets, cov_mat, mu_log, seed) {
  set.seed(seed)
  lhs_raw <- randomLHS(n_sims, n_assets * n_years)
  z_lhs   <- qnorm(lhs_raw)
  L       <- t(chol(cov_mat))

  # 3D array: simulations x years x assets
  ret_array <- array(NA_real_, dim = c(n_sims, n_years, n_assets))

  for (t in 1:n_years) {
    cols     <- ((t - 1) * n_assets + 1):(t * n_assets)
    corr_log <- sweep(z_lhs[, cols] %*% t(L), 2, mu_log, "+")
    ret_array[, t, ] <- exp(corr_log) - 1
  }
  ret_array
}

cat("Generating LHS return matrices and death ages...\n")
t0_lhs <- Sys.time()

set.seed(42)
death_ages <- sample_death_ages(N_MC, mort_table)
ret_array  <- generate_lhs_returns(N_MC, n_years_total, N_ASSETS, cov_mat, mu_log, seed = 123)

elapsed_lhs <- as.numeric(difftime(Sys.time(), t0_lhs, units = "secs"))
cat(sprintf("Done: %s LHS paths x %d assets in %.1f seconds.\n",
            format(N_MC, big.mark=","), N_ASSETS, elapsed_lhs))

cat(sprintf("\nReturn verification:\n"))
for (k in seq_len(N_ASSETS)) {
  cat(sprintf("  %-8s %.3f%% (target %.3f%%)\n",
              paste0(asset_names[k], ":"),
              mean(ret_array[, , k]) * 100, mu_arith[k] * 100))
}
```

# Fitness Function

This is the core of the simulation. The fitness function takes a single candidate solution, a flat vector of length N_ASSETS * N_CP, and returns a measure of how good the resulting multi-asset glide path is.

The candidate vector is first passed to bernstein_weight_paths(), which reshapes it into one Bernstein curve per asset, expands each curve across the full lifespan, and normalizes the resulting values at each year so the portfolio weights sum to one. The lifecycle simulation then runs for each of the N_MC Monte Carlo paths, stepping year by year from the current age to the maximum age. During working years, annual savings contributions are added. The portfolio return each year is the dot product of the weight vector for that year with the vector of asset returns for that year, which is completely generic over the number of asset classes. During retirement, annual spending (net of Social Security benefits) is withdrawn. If wealth drops to zero, the path is marked as ruined. When the simulation reaches the path's randomly assigned death age, the terminal wealth is recorded.

The ruin probability is the fraction of simulation paths that survived to retirement and ran out of money before death, and this is the primary fitness metric (lower is better). P(bequest) is the fraction of paths whose terminal wealth exceeds the bequest target, and median terminal wealth and path roughness are also returned for monitoring but do not enter the fitness score. Roughness is now computed as the sum of squared year-over-year changes across all asset weight curves.

```{r fitness_fn}
make_fitness_fn <- function(ret_array, death_ages, n_sims) {
  function(beta_flat) {
    # Expand to n_years x N_ASSETS weight matrix (rows sum to 1)
    W <- bernstein_weight_paths(beta_flat)

    wealth          <- rep(my_current_savings, n_sims)
    income          <- rep(my_current_income,  n_sims)
    ruined          <- rep(FALSE, n_sims)
    terminal_wealth <- rep(0.0, n_sims)
    da              <- death_ages

    for (t in seq_len(n_years_total)) {
      age    <- my_current_age + t
      active <- (da >= age) & !ruined
      if (!any(active)) break

      if (age <= my_retirement_age) {
        wealth[active] <- wealth[active] + income[active] * my_savings_rate
      }

      # Portfolio return: dot product of weight vector with asset returns
      port_r <- as.numeric(ret_array[, t, ] %*% W[t, ])
      wealth[active] <- wealth[active] * (1 + port_r[active])

      if (age > my_retirement_age) {
        ss_yr  <- ifelse(age >= my_ss_start_age, my_ss_benefit, 0)
        needed <- pmax(my_annual_spending - ss_yr, 0)
        wealth[active] <- wealth[active] - needed
        new_ruin <- active & (wealth <= 0)
        ruined[new_ruin] <- TRUE
        wealth[ruined]   <- 0
      }

      if (age <= my_retirement_age) {
        income[active] <- income[active] * (1 + my_real_income_growth)
      }

      dying <- (da == age) & !ruined
      terminal_wealth[dying] <- wealth[dying]
      terminal_wealth[da == age & ruined] <- 0
    }

    retired <- da > my_retirement_age
    p_ruin  <- if (sum(retired) > 0) mean(ruined[retired]) else 1.0

    # Roughness: sum of squared diffs across all asset weight curves
    roughness <- sum(apply(W, 2, function(col) sum(diff(col)^2)))

    list(
      p_ruin        = p_ruin,
      fitness       = p_ruin,                  # pure P(ruin) -- no penalty
      roughness     = roughness,               # diagnostic only
      weight_paths  = W,
      beta          = pmin(pmax(beta_flat, 0), 1),
      p_bequest     = mean(terminal_wealth >= my_bequest_target),
      median_wealth = median(terminal_wealth)
    )
  }
}

fitness_fn <- make_fitness_fn(ret_array, death_ages, N_MC)

cat(sprintf("Fitness function created.\n"))
cat(sprintf("Search space: %d assets x %d control points = %d dims -> %d-year smooth weight paths.\n",
            N_ASSETS, N_CP, N_DIMS_TOTAL, n_years_total))
```

# CMA-ES Implementation

This chunk implements the full sep-CMA-ES optimization loop. Here is what happens at each generation.

The mean vector m is initialized to sensible starting weights for each asset class. The first N_CP entries (Cash) start at 0.20, the next N_CP (IntBond) at 0.30, and the equity assets at higher values, reflecting a reasonable starting allocation. After normalization by bernstein_weight_paths() these become actual portfolio weights. The diagonal covariance C_diag is set to all ones (isotropic search), the evolution paths p_sigma and p_c are initialized to zero, and sigma starts at CMAES_SIGMA0. Each generation, lambda candidate solutions are drawn from the distribution N(m, sigma^2 * diag(C_diag)) and then clamped to [0,1], with each candidate being a flat vector of length N_DIMS_TOTAL. All lambda candidates are evaluated in parallel on the same pre-generated Monte Carlo paths, where each evaluation expands the control points into a full set of weight paths via the Bernstein polynomial engine, runs the lifecycle simulation, and returns the ruin probability.

After evaluation, candidates are sorted by fitness (P(ruin), lower is better) and the best mu candidates are combined via log-weighted averaging to produce the new mean m. Two evolution paths are then updated. The sigma path (p_sigma) tracks the cumulative displacement of the mean in a coordinate system normalized by the covariance matrix and is used to adapt sigma via cumulative step-size adaptation (CSA): if the path is longer than expected under random selection, sigma is too small and is increased, and if shorter, sigma is too large and is decreased. The covariance path (p_c) tracks displacement in the original coordinate system and is used in the rank-one update of the covariance matrix. The diagonal covariance C_diag is updated with three components: a decay term (shrinking toward the current value), a rank-one term (based on p_c), and a rank-mu term (based on the spread of the selected candidates), which allows the algorithm to learn which control points have more or less influence on fitness and to adjust their exploration ranges accordingly. Sigma is adapted based on the length of the sigma evolution path relative to its expected length under a random walk, which ensures sigma shrinks as the algorithm converges and does not shrink prematurely if the algorithm is still making progress.

The loop ends when any of three conditions is met: sigma falls below 1e-5 (the search has collapsed to a point), the best fitness has not improved for STALL_LIM consecutive generations, or the maximum number of generations is reached. A parallel cluster is created at the start and used for all fitness evaluations throughout the run, and it is automatically shut down when the function exits.

```{r cmaes_engine}
cmaes_optimize <- function(n_dims     = N_DIMS_TOTAL,
                            pop_size   = CMAES_POP_SIZE,
                            max_gen    = CMAES_MAX_GEN,
                            sigma0     = CMAES_SIGMA0,
                            tol        = CMAES_TOL,
                            stall_lim  = CMAES_STALL_LIM,
                            verbose    = TRUE) {

  lambda  <- pop_size
  mu      <- floor(lambda / 2)
  raw_w   <- log(mu + 0.5) - log(1:mu)
  weights <- raw_w / sum(raw_w)
  mu_eff  <- 1 / sum(weights^2)

  c_sigma <- (mu_eff + 2) / (n_dims + mu_eff + 5)
  d_sigma <- 1 + 2 * max(0, sqrt((mu_eff - 1) / (n_dims + 1)) - 1) + c_sigma
  c_c     <- (4 + mu_eff / n_dims) / (n_dims + 4 + 2 * mu_eff / n_dims)
  c_1     <- 2 / ((n_dims + 1.3)^2 + mu_eff)
  c_mu_r  <- min(1 - c_1, 2 * (mu_eff - 2 + 1/mu_eff) / ((n_dims + 2)^2 + mu_eff))
  chi_n   <- sqrt(n_dims) * (1 - 1/(4*n_dims) + 1/(21*n_dims^2))

  # Initialize: sensible starting raw values per asset (will be normalized)
  init_vals <- c(0.20, 0.30, 0.80, 0.50)  # Cash, IntBond, DomEq, IntlEq
  m <- rep(init_vals, each = N_CP)

  sigma   <- sigma0
  C_diag  <- rep(1.0, n_dims)
  p_sigma <- rep(0.0, n_dims)
  p_c     <- rep(0.0, n_dims)

  best_fitness_ever <- Inf
  best_ruin_ever    <- Inf
  best_beta_ever    <- m
  best_details_ever <- NULL
  history           <- data.frame()
  stall_count       <- 0
  prev_best         <- Inf

  cat(sprintf("  sep-CMA-ES: %d assets x %d CPs = %d dims, lambda=%d, mu=%d, mu_eff=%.1f\n",
              N_ASSETS, N_CP, n_dims, lambda, mu, mu_eff))
  cat(sprintf("  MC budget: %s fixed LHS paths per evaluation\n",
              format(N_MC, big.mark=",")))
  cat(sprintf("  Parallel cores: %d\n\n", N_CORES))

  cl <- makeCluster(N_CORES)
  on.exit(stopCluster(cl), add = TRUE)

  clusterExport(cl, varlist = c(
    "make_fitness_fn", "bernstein_weight_paths", "bernstein_path", "de_casteljau",
    "ret_array", "death_ages",
    "n_years_total", "N_CP", "N_MC", "N_ASSETS",
    "my_current_age", "my_retirement_age", "my_ss_start_age",
    "my_current_savings", "my_current_income", "my_savings_rate",
    "my_real_income_growth", "my_annual_spending",
    "my_ss_benefit", "my_bequest_target",
    "fitness_fn"
  ), envir = environment())

  t0 <- Sys.time()

  for (gen in 1:max_gen) {

    # Sample lambda candidates; clamp to [0,1]
    candidates <- matrix(NA_real_, nrow = lambda, ncol = n_dims)
    for (k in 1:lambda) {
      z <- rnorm(n_dims)
      candidates[k, ] <- pmin(pmax(m + sigma * sqrt(C_diag) * z, 0), 1)
    }

    # Parallel fitness evaluation -- same fixed MC budget every generation
    cand_list    <- lapply(seq_len(lambda), function(k) candidates[k, ])
    results_list <- parLapply(cl, cand_list, function(cand) fitness_fn(cand))

    fitvals   <- sapply(results_list, `[[`, "fitness")
    ruin_vals <- sapply(results_list, `[[`, "p_ruin")
    details   <- results_list

    ord        <- order(fitvals)
    fitvals    <- fitvals[ord]
    ruin_vals  <- ruin_vals[ord]
    candidates <- candidates[ord, , drop = FALSE]
    details    <- details[ord]

    m_old <- m
    m     <- as.numeric(weights %*% candidates[1:mu, , drop = FALSE])

    invsqrt_C <- 1 / sqrt(C_diag)
    step      <- (m - m_old) / sigma

    p_sigma <- (1 - c_sigma) * p_sigma +
               sqrt(c_sigma * (2 - c_sigma) * mu_eff) * invsqrt_C * step

    h_sigma <- as.numeric(
      sum(p_sigma^2) / n_dims / (1 - (1 - c_sigma)^(2*(gen+1))) < 2 + 4/(n_dims+1)
    )

    p_c <- (1 - c_c) * p_c +
           h_sigma * sqrt(c_c * (2 - c_c) * mu_eff) * step

    artmp  <- (candidates[1:mu, , drop = FALSE] - rep(m_old, each = mu)) / sigma
    C_diag <- (1 - c_1 - c_mu_r) * C_diag +
              c_1 * (p_c^2 + (1 - h_sigma) * c_c * (2 - c_c) * C_diag) +
              c_mu_r * as.numeric(weights %*% artmp^2)
    C_diag <- pmax(C_diag, 1e-20)

    sigma <- sigma * exp((c_sigma / d_sigma) * (sqrt(sum(p_sigma^2)) / chi_n - 1))
    sigma <- min(sigma, 1.0)

    gen_best_fit  <- fitvals[1]
    gen_best_ruin <- ruin_vals[1]
    gen_best_det  <- details[[1]]

    if (gen_best_fit < best_fitness_ever) {
      best_fitness_ever <- gen_best_fit
      best_ruin_ever    <- gen_best_ruin
      best_beta_ever    <- candidates[1, ]
      best_details_ever <- gen_best_det
    }

    if (abs(prev_best - gen_best_fit) < tol) {
      stall_count <- stall_count + 1
    } else {
      stall_count <- 0
    }
    prev_best <- gen_best_fit

    history <- bind_rows(history, data.frame(
      gen            = gen,
      best_ruin      = gen_best_ruin,
      mean_ruin      = mean(ruin_vals),
      best_fit       = gen_best_fit,
      mean_fit       = mean(fitvals),
      roughness      = gen_best_det$roughness,
      best_beq       = gen_best_det$p_bequest,
      median_w       = gen_best_det$median_wealth,
      sigma          = sigma,
      best_ever_ruin = best_ruin_ever
    ))

    if (verbose && (gen %% 10 == 0 || gen <= 5 || gen == max_gen)) {
      elapsed <- as.numeric(difftime(Sys.time(), t0, units = "mins"))
      cat(sprintf("  Gen %3d | P(ruin)=%.3f%% | P(beq)=%.1f%% | rough=%.4f | sig=%.4f | %.1fmin\n",
                  gen, gen_best_ruin * 100,
                  gen_best_det$p_bequest * 100, gen_best_det$roughness,
                  sigma, elapsed))
    }

    if (stall_count >= stall_lim) {
      cat(sprintf("\n  Converged at generation %d (stalled %d gens)\n",
                  gen, stall_lim))
      break
    }
    if (sigma < 1e-5) {
      cat(sprintf("\n  Converged at generation %d (sigma collapsed to %.2e)\n", gen, sigma))
      break
    }
  }

  elapsed_total <- as.numeric(difftime(Sys.time(), t0, units = "mins"))

  final <- fitness_fn(best_beta_ever)

  cat(sprintf("\n  CMA-ES complete: %d generations in %.1f minutes\n", gen, elapsed_total))
  cat(sprintf("  Final P(ruin):    %.2f%%\n", final$p_ruin * 100))
  cat(sprintf("  Final P(bequest): %.1f%%\n", final$p_bequest * 100))
  cat(sprintf("  Final median $:   $%s\n", format(round(final$median_wealth), big.mark=",")))
  cat(sprintf("  Path roughness:   %.4f\n", final$roughness))

  list(
    best_beta     = final$beta,
    best_weights  = final$weight_paths,
    best_fitness  = final$fitness,
    best_ruin     = final$p_ruin,
    best_details  = final,
    history       = history,
    n_gens        = gen,
    elapsed_min   = elapsed_total
  )
}
```

# Run Optimization

This chunk calls the optimizer. Everything above -- the pre-generated returns, the fitness function, the CMA-ES engine -- comes together here. The optimizer will print progress every 10 generations showing the current best P(ruin), P(bequest), roughness, sigma, and elapsed time. When it converges (via stall detection or sigma collapse), it re-evaluates the best solution one final time and returns the complete results.

```{r run_cmaes, cache=FALSE}
cat("========== STARTING CMA-ES OPTIMIZATION (N-ASSET BERNSTEIN) ==========\n\n")
result <- cmaes_optimize()
```

# Optimized Glide Path

This is the main output: the full N-asset allocation glide path that CMA-ES discovered. The stacked area chart shows the portfolio weight of every asset class at every age from the current age to the maximum lifespan. Each asset's allocation over time is a smooth Bernstein polynomial curve, and the normalization ensures the weights sum to one at every age. Reference lines mark the retirement age and median death age to provide context for interpreting the shape.

```{r glide_path, fig.width=10, fig.height=6}
W <- result$best_weights

path_df <- data.frame(age = (my_current_age + 1):max_age, W)
colnames(path_df) <- c("age", asset_names)

path_df %>%
  pivot_longer(cols = all_of(asset_names), names_to = "asset", values_to = "weight") %>%
  mutate(asset = factor(asset, levels = asset_names)) %>%
  ggplot(aes(x = age, y = weight, fill = asset)) +
  geom_area(alpha = 0.85) +
  geom_vline(xintercept = my_retirement_age,        linetype = "dashed", color = "grey30") +
  geom_vline(xintercept = mc_mort$median_death_age, linetype = "dotted", color = "grey10") +
  annotate("text", x = my_retirement_age + 1, y = 0.97,
           label = paste("Retire:", my_retirement_age), hjust = 0, size = 3.5) +
  annotate("text", x = mc_mort$median_death_age + 1, y = 0.85,
           label = paste("Median death:", round(mc_mort$median_death_age, 1)),
           hjust = 0, size = 3.5) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(labels = percent_format()) +
  labs(title = "CMA-ES Optimized N-Asset Lifetime Glide Path (Bernstein Polynomial)",
       subtitle = sprintf("P(ruin)=%.2f%% | P(bequest>=$%sk)=%.1f%% | Median wealth=$%s | %d assets x %d CPs",
                          result$best_ruin * 100,
                          format(my_bequest_target / 1000, big.mark = ","),
                          result$best_details$p_bequest * 100,
                          format(round(result$best_details$median_wealth), big.mark = ","),
                          N_ASSETS, N_CP),
       x = "Age", y = "Portfolio Weight", fill = "",
       caption = "Each asset class has its own independent Bernstein curve, normalized to sum to 1") +
  theme_minimal(base_size = 14)
```

# Individual Asset Curves

This plot shows each asset's weight as a separate line, making it easier to read the exact allocation at any given age than the stacked area chart above.

```{r individual_curves, fig.width=10, fig.height=5}
path_df %>%
  pivot_longer(cols = all_of(asset_names), names_to = "asset", values_to = "weight") %>%
  mutate(asset = factor(asset, levels = asset_names)) %>%
  ggplot(aes(x = age, y = weight, color = asset)) +
  geom_line(linewidth = 1.1) +
  geom_vline(xintercept = my_retirement_age, linetype = "dashed", color = "grey40") +
  scale_y_continuous(labels = percent_format(), limits = c(0, NA)) +
  scale_color_brewer(palette = "Set2") +
  labs(title = "Individual Asset Weight Curves",
       x = "Age", y = "Portfolio Weight", color = "") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

# Control Point Values

This table lists the optimized Bernstein control points for each asset class. Each control point is associated with a life-stage age (spaced evenly across the lifespan). The raw column shows the pre-normalization control point value that CMA-ES optimized; the actual portfolio weight at any age is determined by expanding all curves and normalizing. Post-retirement control points are marked with an asterisk.

```{r cp_table}
cp_age_labels <- round(seq(my_current_age + 1, max_age, length.out = N_CP))
beta_mat <- matrix(result$best_beta, nrow = N_CP, ncol = N_ASSETS)

cat("Optimized Bernstein Control Points (raw, pre-normalization)\n")
cat("(* = post-retirement)\n\n")

for (k in seq_len(N_ASSETS)) {
  cat(sprintf("--- %s ---\n", asset_names[k]))
  cp_tbl <- data.frame(
    CP  = paste0("B", seq(0, N_CP - 1)),
    Age = cp_age_labels,
    Raw = sprintf("%.3f", beta_mat[, k]),
    stringsAsFactors = FALSE
  )
  cp_tbl$CP[cp_tbl$Age >= my_retirement_age] <-
    paste0(cp_tbl$CP[cp_tbl$Age >= my_retirement_age], " *")
  print(cp_tbl, row.names = FALSE)
  cat("\n")
}
```

# Diagnostics

The following plots track the internal state of the CMA-ES optimizer over the course of the run. They are useful for verifying that the algorithm behaved well, that it converged smoothly, did not get stuck, and did not terminate prematurely.

## Convergence

This plot shows three traces of P(ruin) over generations: the best candidate in each generation, the mean across all candidates in each generation, and the best-ever value seen up to that point. The best-ever trace should decrease monotonically (by definition). The gap between the generation mean and the generation best reflects the diversity of the population, and as the algorithm converges all three traces should come together.

```{r convergence, fig.height=6}
ggplot(result$history, aes(x = gen)) +
  geom_line(aes(y = best_ruin * 100,      color = "Gen best P(ruin)"), linewidth = 0.8) +
  geom_line(aes(y = mean_ruin * 100,      color = "Gen mean P(ruin)"), linewidth = 0.5, alpha = 0.5) +
  geom_line(aes(y = best_ever_ruin * 100, color = "Best ever P(ruin)"), linewidth = 1.2) +
  scale_color_manual(values = c("Gen best P(ruin)"  = "steelblue",
                                 "Gen mean P(ruin)"  = "grey60",
                                 "Best ever P(ruin)" = "firebrick")) +
  labs(title = "CMA-ES Convergence",
       subtitle = sprintf("Final P(ruin) = %.2f%%  |  %s fixed LHS paths throughout",
                          result$best_ruin * 100, format(N_MC, big.mark=",")),
       x = "Generation", y = "P(Ruin) %", color = "") +
  theme_minimal(base_size = 14)
```

## Step Size (Sigma) Evolution

Sigma is the global step size of the CMA-ES search distribution and controls how far from the current mean the algorithm samples new candidates. Early in the run sigma should remain relatively large as the algorithm explores, and as it hones in on a solution sigma should decrease. A sharp collapse of sigma toward zero is a strong signal that the algorithm has converged to a tight region of the search space and further generations would not produce meaningful improvement. Because we are working in control-point space (where each dimension is in [0,1]), the scale of sigma is directly interpretable: a sigma of 0.01 means the algorithm is exploring variations of roughly +/- 1% in each control point.

```{r sigma_plot, fig.height=4}
ggplot(result$history, aes(x = gen, y = sigma)) +
  geom_line(color = "purple4", linewidth = 0.8) +
  labs(title = "Step Size (Sigma) in Control-Point Space",
       subtitle = "Collapse indicates tight convergence on the optimal allocation shape",
       x = "Generation", y = "Sigma") +
  theme_minimal(base_size = 14)
```

## Path Roughness (Diagnostic Only)

Roughness is defined as the sum of squared year-over-year changes across all asset weight curves. It is not part of the fitness function and the optimizer does not try to minimize it. It is tracked purely as a sanity check to verify that the Bernstein polynomial parameterization is doing its job of keeping the paths smooth. With Bernstein polynomials, roughness should be low and stable from generation 1 because the polynomial basis makes jaggedness structurally impossible.

```{r roughness_plot, fig.height=4}
ggplot(result$history, aes(x = gen, y = roughness)) +
  geom_line(color = "darkgreen", linewidth = 0.8) +
  labs(title = "Path Roughness Over Generations -- Diagnostic Only, Not Penalized",
       subtitle = sprintf("Final roughness: %.4f", tail(result$history$roughness, 1)),
       x = "Generation", y = "Sum of squared diffs (all assets)") +
  theme_minimal(base_size = 14)
```

## P(Ruin) vs P(Bequest) Over Generations

This plot tracks the co-evolution of P(ruin) and P(bequest) over the course of the optimization. Since the optimizer is minimizing only P(ruin), P(bequest) is a free observable that goes wherever the ruin-minimizing shape takes it. In practice there is often a positive relationship between low ruin probability and high bequest probability because both benefit from strategies that grow wealth effectively during working years. But the relationship is not perfect, and this plot helps reveal whether the optimizer is making trade-offs between the two.

```{r ruin_bequest_evolution, fig.height=5}
result$history %>%
  dplyr::select(gen, best_ruin, best_beq) %>%
  pivot_longer(-gen, names_to = "metric", values_to = "value") %>%
  mutate(metric = ifelse(metric == "best_ruin", "P(Ruin)", "P(Bequest)")) %>%
  ggplot(aes(x = gen, y = value * 100, color = metric)) +
  geom_line(linewidth = 1) +
  scale_color_manual(values = c("P(Ruin)" = "firebrick", "P(Bequest)" = "steelblue")) +
  labs(title = "P(Ruin) and P(Bequest) Over Generations",
       x = "Generation", y = "%", color = "") +
  theme_minimal(base_size = 14)
```

## Summary Statistics

This final block prints the full configuration and results in plain text for easy reference and reproducibility.

```{r summary_table}
config_data <- tibble(
  Parameter = c(
    "Algorithm",
    "Parameterization",
    "Asset classes",
    "Control points per asset (N_CP)",
    "Total optimization dimensions",
    "Shape constraint",
    "Roughness penalty",
    "MC sampling",
    "MC paths (fixed)",
    "Population (lambda)",
    "Parallel cores"
  ),
  Value = c(
    "sep-CMA-ES (diagonal covariance)",
    "Bernstein polynomial via de Casteljau",
    sprintf("%d (%s)", N_ASSETS, paste(asset_names, collapse = ", ")),
    as.character(N_CP),
    sprintf("%d  (= %d assets x %d CPs)", N_DIMS_TOTAL, N_ASSETS, N_CP),
    "Infinitely smooth -- structurally enforced",
    "None required",
    "Latin Hypercube Sampling",
    format(N_MC, big.mark = ","),
    as.character(CMAES_POP_SIZE),
    as.character(N_CORES)
  )
)

results_data <- tibble(
  Parameter = c(
    "Generations run",
    "Final sigma (control-point space)",
    "Runtime (min)",
    "P(Ruin)",
    "P(Bequest >= target)",
    "Median terminal wealth",
    "Path roughness (diagnostic)"
  ),
  Value = c(
    as.character(result$n_gens),
    sprintf("%.6f", tail(result$history$sigma, 1)),
    sprintf("%.1f", result$elapsed_min),
    sprintf("%.2f%%", result$best_ruin * 100),
    sprintf("%.1f%%",  result$best_details$p_bequest * 100),
    sprintf("$%s", format(round(result$best_details$median_wealth), big.mark = ",")),
    sprintf("%.4f", result$best_details$roughness)
  )
)

cat("=== Configuration ===\n")
for (i in seq_len(nrow(config_data))) {
  cat(sprintf("  %-35s %s\n", config_data$Parameter[i], config_data$Value[i]))
}

cat("\n=== Optimization Results ===\n")
for (i in seq_len(nrow(results_data))) {
  cat(sprintf("  %-35s %s\n", results_data$Parameter[i], results_data$Value[i]))
}
```

# References

Hansen, N. (2016). The CMA Evolution Strategy: A Tutorial. *arXiv:1604.00772*.

Lorentz, G.G. (1953). *Bernstein Polynomials.* University of Toronto Press.

McKay, M. D., Beckman, R. J., & Conover, W. J. (1979). A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code. *Technometrics*, 21(2), 239--245.

*Optimization: `r CMAES_POP_SIZE` offspring x `r result$n_gens` generations = `r format(CMAES_POP_SIZE * result$n_gens, big.mark=",")` fitness evaluations. Each evaluation expands `r N_ASSETS` x `r N_CP` Bernstein control points into `r n_years_total`-year smooth weight paths and simulates `r format(N_MC, big.mark=",")` fixed LHS paths.*
